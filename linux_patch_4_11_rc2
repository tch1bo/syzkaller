diff --git a/Makefile b/Makefile
index b841fb36beb2..546761942766 100644
--- a/Makefile
+++ b/Makefile
@@ -374,6 +374,7 @@ AFLAGS_KERNEL	=
 LDFLAGS_vmlinux =
 CFLAGS_GCOV	= -fprofile-arcs -ftest-coverage -fno-tree-loop-im -Wno-maybe-uninitialized
 CFLAGS_KCOV	:= $(call cc-option,-fsanitize-coverage=trace-pc,)
+CFLAGS_KCOV	+= $(call cc-option,-fsanitize-coverage=trace-cmp,)
 
 
 # Use USERINCLUDE when you must reference the UAPI directories only.
@@ -629,7 +630,9 @@ ARCH_AFLAGS :=
 ARCH_CFLAGS :=
 include arch/$(SRCARCH)/Makefile
 
+ifneq ($(cc-name),clang)
 KBUILD_CFLAGS	+= $(call cc-option,-fno-delete-null-pointer-checks,)
+endif
 KBUILD_CFLAGS	+= $(call cc-disable-warning,frame-address,)
 
 ifdef CONFIG_LD_DEAD_CODE_DATA_ELIMINATION
@@ -704,6 +707,9 @@ KBUILD_CFLAGS += $(call cc-disable-warning, tautological-compare)
 # See modpost pattern 2
 KBUILD_CFLAGS += $(call cc-option, -mno-global-merge,)
 KBUILD_CFLAGS += $(call cc-option, -fcatch-undefined-behavior)
+
+KBUILD_CFLAGS += $(call cc-option, -no-integrated-as)
+KBUILD_AFLAGS += $(call cc-option, -no-integrated-as)
 else
 
 # These warnings generated too much noise in a regular build.
diff --git a/arch/x86/Makefile b/arch/x86/Makefile
index 2d449337a360..894a8d18bf97 100644
--- a/arch/x86/Makefile
+++ b/arch/x86/Makefile
@@ -87,11 +87,13 @@ else
         KBUILD_AFLAGS += -m64
         KBUILD_CFLAGS += -m64
 
+ifneq ($(cc-name),clang)
         # Align jump targets to 1 byte, not the default 16 bytes:
         KBUILD_CFLAGS += -falign-jumps=1
 
         # Pack loops tightly as well:
         KBUILD_CFLAGS += -falign-loops=1
+endif
 
         # Don't autogenerate traditional x87 instructions
         KBUILD_CFLAGS += $(call cc-option,-mno-80387)
diff --git a/arch/x86/boot/memory.c b/arch/x86/boot/memory.c
index db75d07c3645..7af65046dfad 100644
--- a/arch/x86/boot/memory.c
+++ b/arch/x86/boot/memory.c
@@ -63,8 +63,13 @@ static int detect_memory_e820(void)
 			count = 0;
 			break;
 		}
-
+#ifdef __clang__
+		/* PR18415 */
+		memcpy(desc, &buf, sizeof(*desc));
+		desc++;
+#else
 		*desc++ = buf;
+#endif
 		count++;
 	} while (ireg.ebx && count < ARRAY_SIZE(boot_params.e820_map));
 
diff --git a/arch/x86/boot/string.h b/arch/x86/boot/string.h
index 113588ddb43f..856cd1c85c8d 100644
--- a/arch/x86/boot/string.h
+++ b/arch/x86/boot/string.h
@@ -14,9 +14,11 @@ int memcmp(const void *s1, const void *s2, size_t len);
  * Access builtin version by default. If one needs to use optimized version,
  * do "undef memcpy" in .c file and link against right string.c
  */
+#ifndef __clang__ /* PR18415 */
 #define memcpy(d,s,l) __builtin_memcpy(d,s,l)
 #define memset(d,c,l) __builtin_memset(d,c,l)
 #define memcmp	__builtin_memcmp
+#endif
 
 extern int strcmp(const char *str1, const char *str2);
 extern int strncmp(const char *cs, const char *ct, size_t count);
diff --git a/arch/x86/include/asm/uaccess.h b/arch/x86/include/asm/uaccess.h
index ea148313570f..ce1b30c3f18c 100644
--- a/arch/x86/include/asm/uaccess.h
+++ b/arch/x86/include/asm/uaccess.h
@@ -162,11 +162,13 @@ __typeof__(__builtin_choose_expr(sizeof(x) > sizeof(0UL), 0ULL, 0UL))
  * Clang/LLVM cares about the size of the register, but still wants
  * the base register for something that ends up being a pair.
  */
+
+register unsigned long int __sp asm(_ASM_SP);
+
 #define get_user(x, ptr)						\
 ({									\
 	int __ret_gu;							\
 	register __inttype(*(ptr)) __val_gu asm("%"_ASM_DX);		\
-	register void *__sp asm(_ASM_SP);				\
 	__chk_user_ptr(ptr);						\
 	might_fault();							\
 	asm volatile("call __get_user_%P4"				\
diff --git a/arch/x86/kernel/Makefile b/arch/x86/kernel/Makefile
index 84c00592d359..2a10228cc656 100644
--- a/arch/x86/kernel/Makefile
+++ b/arch/x86/kernel/Makefile
@@ -25,6 +25,8 @@ KASAN_SANITIZE_dumpstack.o				:= n
 KASAN_SANITIZE_dumpstack_$(BITS).o			:= n
 KASAN_SANITIZE_stacktrace.o := n
 
+KASAN_SANITIZE_unwind_frame.o := n
+
 OBJECT_FILES_NON_STANDARD_head_$(BITS).o		:= y
 OBJECT_FILES_NON_STANDARD_relocate_kernel_$(BITS).o	:= y
 OBJECT_FILES_NON_STANDARD_mcount_$(BITS).o		:= y
diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index 428e31763cb9..bb9bd8b00027 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -1211,6 +1211,7 @@ static inline bool smap_violation(int error_code, struct pt_regs *regs)
  * {,trace_}do_page_fault() have notrace on. Having this an actual function
  * guarantees there's a function trace entry.
  */
+__attribute__((no_sanitize("kernel-address")))
 static noinline void
 __do_page_fault(struct pt_regs *regs, unsigned long error_code,
 		unsigned long address)
diff --git a/include/linux/compiler-clang.h b/include/linux/compiler-clang.h
index de179993e039..9c4cae3f7e84 100644
--- a/include/linux/compiler-clang.h
+++ b/include/linux/compiler-clang.h
@@ -15,3 +15,7 @@
  * with any version that can compile the kernel
  */
 #define __UNIQUE_ID(prefix) __PASTE(__PASTE(__UNIQUE_ID_, prefix), __COUNTER__)
+
+/* all clang versions usable with the kernel support KASAN ABI version 5
+ */
+#define KASAN_ABI_VERSION 5
diff --git a/include/linux/kcov.h b/include/linux/kcov.h
index 2883ac98c280..b5aefd00739e 100644
--- a/include/linux/kcov.h
+++ b/include/linux/kcov.h
@@ -11,15 +11,36 @@ void kcov_task_init(struct task_struct *t);
 void kcov_task_exit(struct task_struct *t);
 
 enum kcov_mode {
-	/* Coverage collection is not enabled yet. */
-	KCOV_MODE_DISABLED = 0,
-	/*
-	 * Tracing coverage collection mode.
-	 * Covered PCs are collected in a per-task buffer.
-	 */
-	KCOV_MODE_TRACE = 1,
+    /* Coverage collection is not enabled yet. */
+    KCOV_MODE_DISABLED = 0,
+    /*
+     * Tracing coverage collection mode.
+     * Covered PCs are collected in a per-task buffer.
+     */
+    KCOV_MODE_TRACE_PC = 1,
+    /*
+     * Collecting comparison operands mode.
+     */
+    KCOV_MODE_TRACE_CMP = 2,
+
+    KCOV_MODE_INIT = 3,
 };
 
+enum kcov_coverage_type {
+    KCOV_TYPE_PC,
+    KCOV_TYPE_CMP1,
+    KCOV_TYPE_CMP2,
+    KCOV_TYPE_CMP4,
+    KCOV_TYPE_CMP8,
+    KCOV_TYPE_SWITCH1,
+    KCOV_TYPE_SWITCH2,
+    KCOV_TYPE_SWITCH4,
+    KCOV_TYPE_SWITCH8,
+    KCOV_TYPE_CONST_CMP1,
+    KCOV_TYPE_CONST_CMP2,
+    KCOV_TYPE_CONST_CMP4,
+    KCOV_TYPE_CONST_CMP8,
+};
 #else
 
 static inline void kcov_task_init(struct task_struct *t) {}
diff --git a/include/linux/kernel.h b/include/linux/kernel.h
index 4c26dc3a8295..9406eea811ad 100644
--- a/include/linux/kernel.h
+++ b/include/linux/kernel.h
@@ -852,6 +852,21 @@ static inline void ftrace_dump(enum ftrace_dump_mode oops_dump_mode) { }
 	const typeof( ((type *)0)->member ) *__mptr = (ptr);	\
 	(type *)( (char *)__mptr - offsetof(type,member) );})
 
+/**
+ * container_of_safe - safe version of container_of
+ * @ptr:	the pointer to the member.
+ * @type:	the type of the container struct this is embedded in.
+ * @member:	the name of the member within the struct.
+ *
+ * In the case the value of @ptr is smaller than the offset of @member within
+ * @type, return 0.
+ */
+#define container_of_safe(ptr, type, member) ({			\
+	const typeof( ((type *)0)->member ) *__mptr = (ptr);	\
+        (size_t)__mptr >= offsetof(type,member) ?		\
+	(type *)( (char *)__mptr - offsetof(type,member) ) : (type *)0 ;})
+
+
 /* Rebuild everything on CONFIG_FTRACE_MCOUNT_RECORD */
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD
 # define REBUILD_DUE_TO_FTRACE_MCOUNT_RECORD
diff --git a/include/linux/llist.h b/include/linux/llist.h
index 171baa90f6f6..d8b7e9dda5cc 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -92,6 +92,16 @@ static inline void init_llist_head(struct llist_head *list)
 #define llist_entry(ptr, type, member)		\
 	container_of(ptr, type, member)
 
+/**
+ * llist_entry_safe - get the struct of this entry without overflowing
+ * @ptr:	the &struct llist_node pointer.
+ * @type:	the type of the struct this is embedded in.
+ * @member:	the name of the llist_node within the struct.
+ */
+#define llist_entry_safe(ptr, type, member)		\
+	container_of_safe(ptr, type, member)
+
+
 /**
  * llist_for_each - iterate over some deleted entries of a lock-less list
  * @pos:	the &struct llist_node to use as a loop cursor
@@ -125,9 +135,10 @@ static inline void init_llist_head(struct llist_head *list)
  * reverse the order by yourself before traversing.
  */
 #define llist_for_each_entry(pos, node, member)				\
-	for ((pos) = llist_entry((node), typeof(*(pos)), member);	\
-	     &(pos)->member != NULL;					\
-	     (pos) = llist_entry((pos)->member.next, typeof(*(pos)), member))
+	for ((pos) = llist_entry_safe((node), typeof(*(pos)), member);	\
+	     pos != NULL && &(pos)->member != NULL;			\
+	     (pos) = llist_entry_safe((pos)->member.next, \
+					typeof(*(pos)), member))
 
 /**
  * llist_for_each_entry_safe - iterate over some deleted entries of lock-less list of given type
@@ -146,10 +157,11 @@ static inline void init_llist_head(struct llist_head *list)
  * you want to traverse from the oldest to the newest, you must
  * reverse the order by yourself before traversing.
  */
-#define llist_for_each_entry_safe(pos, n, node, member)			       \
-	for (pos = llist_entry((node), typeof(*pos), member);		       \
-	     &pos->member != NULL &&					       \
-	        (n = llist_entry(pos->member.next, typeof(*n), member), true); \
+#define llist_for_each_entry_safe(pos, n, node, member)			     \
+	for (pos = llist_entry_safe((node), typeof(*pos), member);	     \
+	     pos != NULL && &pos->member != NULL &&			     \
+		(n = llist_entry_safe(pos->member.next, typeof(*n), member), \
+		 true); \
 	     pos = n)
 
 /**
diff --git a/kernel/kcov.c b/kernel/kcov.c
index 85e5546cd791..e087d0474219 100644
--- a/kernel/kcov.c
+++ b/kernel/kcov.c
@@ -15,6 +15,7 @@
 #include <linux/sched.h>
 #include <linux/slab.h>
 #include <linux/spinlock.h>
+#include <linux/string.h>
 #include <linux/vmalloc.h>
 #include <linux/debugfs.h>
 #include <linux/uaccess.h>
@@ -30,269 +31,357 @@
  *  - then, repeated enable/disable for a task (only one task a time allowed)
  */
 struct kcov {
-	/*
-	 * Reference counter. We keep one for:
-	 *  - opened file descriptor
-	 *  - task with enabled coverage (we can't unwire it from another task)
-	 */
-	atomic_t		refcount;
-	/* The lock protects mode, size, area and t. */
-	spinlock_t		lock;
-	enum kcov_mode		mode;
-	/* Size of arena (in long's for KCOV_MODE_TRACE). */
-	unsigned		size;
-	/* Coverage buffer shared with user space. */
-	void			*area;
-	/* Task for which we collect coverage, or NULL. */
-	struct task_struct	*t;
+    /*
+     * Reference counter. We keep one for:
+     *  - opened file descriptor
+     *  - task with enabled coverage (we can't unwire it from another task)
+     */
+    atomic_t		refcount;
+    /* The lock protects mode, size, area and t. */
+    spinlock_t		lock;
+    enum kcov_mode		mode;
+    /* Size of arena (in long's for KCOV_MODE_TRACE_PC). */
+    unsigned		size;
+    /* Coverage buffer shared with user space. */
+    void			*area;
+    /* Task for which we collect coverage, or NULL. */
+    struct task_struct	*t;
 };
 
-/*
- * Entry point from instrumented code.
- * This is called once per basic-block/edge.
- */
-void notrace __sanitizer_cov_trace_pc(void)
-{
-	struct task_struct *t;
-	enum kcov_mode mode;
-
-	t = current;
-	/*
-	 * We are interested in code coverage as a function of a syscall inputs,
-	 * so we ignore code executed in interrupts.
-	 * The checks for whether we are in an interrupt are open-coded, because
-	 * 1. We can't use in_interrupt() here, since it also returns true
-	 *    when we are inside local_bh_disable() section.
-	 * 2. We don't want to use (in_irq() | in_serving_softirq() | in_nmi()),
-	 *    since that leads to slower generated code (three separate tests,
-	 *    one for each of the flags).
-	 */
-	if (!t || (preempt_count() & (HARDIRQ_MASK | SOFTIRQ_OFFSET
-							| NMI_MASK)))
-		return;
-	mode = READ_ONCE(t->kcov_mode);
-	if (mode == KCOV_MODE_TRACE) {
-		unsigned long *area;
-		unsigned long pos;
-		unsigned long ip = _RET_IP_;
+static int check_kcov_enabled(enum kcov_mode needed_mode) {
+    struct task_struct *t;
+    enum kcov_mode mode;
+    t = current;
+    if (!t || (preempt_count() & (HARDIRQ_MASK | SOFTIRQ_OFFSET
+                    | NMI_MASK)))
+        return -1;
+    mode = READ_ONCE(t->kcov_mode);
+    if (mode != needed_mode) {
+        return -1;
+    }
+    return 0;
+}
+
+void notrace __sanitizer_cov_trace_pc(void) {
+    struct task_struct *t;
+    unsigned long *area;
+    unsigned long pc = _RET_IP_;
+    unsigned long pos;
 
+    if (check_kcov_enabled(KCOV_MODE_TRACE_PC) < 0) {
+        return;
+    }
+
+    t = current;
 #ifdef CONFIG_RANDOMIZE_BASE
-		ip -= kaslr_offset();
+    pc -= kaslr_offset();
 #endif
 
-		/*
-		 * There is some code that runs in interrupts but for which
-		 * in_interrupt() returns false (e.g. preempt_schedule_irq()).
-		 * READ_ONCE()/barrier() effectively provides load-acquire wrt
-		 * interrupts, there are paired barrier()/WRITE_ONCE() in
-		 * kcov_ioctl_locked().
-		 */
-		barrier();
-		area = t->kcov_area;
-		/* The first word is number of subsequent PCs. */
-		pos = READ_ONCE(area[0]) + 1;
-		if (likely(pos < t->kcov_size)) {
-			area[pos] = ip;
-			WRITE_ONCE(area[0], pos);
-		}
-	}
+    barrier();
+    area = t->kcov_area;
+    pos = READ_ONCE(area[0]) + 1;
+    if (likely(pos < t->kcov_size)) {
+        area[pos] = pc;
+        WRITE_ONCE(area[0], pos);
+    }
+}
+
+static void write_comp_data(enum kcov_coverage_type type,
+        uint64_t arg1, uint64_t arg2) {
+    struct task_struct *t;
+    unsigned long *area;
+    unsigned long count, start_pos, end_pos;
+
+    if (check_kcov_enabled(KCOV_MODE_TRACE_CMP) < 0) {
+        return;
+    }
+
+    t = current;
+    barrier();
+    area = t->kcov_area;
+    count = READ_ONCE(area[0]);
+#define WORDS_PER_CMP 3
+    start_pos = 1 + count*WORDS_PER_CMP;
+    end_pos = start_pos + WORDS_PER_CMP - 1;
+    // every record is WORDS_PER_CMP words
+    if (likely(end_pos < t->kcov_size)) {
+        area[start_pos] = (uint64_t) type;
+        area[start_pos + 1] = arg1;
+        area[start_pos + 2] = arg2;
+        WRITE_ONCE(area[0], count+1);
+    }
+}
+
+void notrace __sanitizer_cov_trace_cmp1(uint8_t arg1, uint8_t arg2) {
+    write_comp_data(KCOV_TYPE_CMP1, arg1, arg2);
+}
+
+void notrace __sanitizer_cov_trace_cmp2(uint16_t arg1, uint16_t arg2) {
+    write_comp_data(KCOV_TYPE_CMP2, arg1, arg2);
+}
+
+void notrace __sanitizer_cov_trace_cmp4(uint32_t arg1, uint32_t arg2) {
+    write_comp_data(KCOV_TYPE_CMP4, arg1, arg2);
+}
+
+void notrace __sanitizer_cov_trace_cmp8(uint64_t arg1, uint64_t arg2) {
+    write_comp_data(KCOV_TYPE_CMP8, arg1, arg2);
+}
+
+void notrace __sanitizer_cov_trace_const_cmp1(uint8_t arg1, uint8_t arg2) {
+    write_comp_data(KCOV_TYPE_CONST_CMP1, arg1, arg2);
+}
+
+void notrace __sanitizer_cov_trace_const_cmp2(uint16_t arg1, uint16_t arg2) {
+    write_comp_data(KCOV_TYPE_CONST_CMP2, arg1, arg2);
+}
+
+void notrace __sanitizer_cov_trace_const_cmp4(uint32_t arg1, uint32_t arg2) {
+    write_comp_data(KCOV_TYPE_CONST_CMP4, arg1, arg2);
+}
+
+void notrace __sanitizer_cov_trace_const_cmp8(uint64_t arg1, uint64_t arg2) {
+    write_comp_data(KCOV_TYPE_CONST_CMP8, arg1, arg2);
+}
+
+void notrace __sanitizer_cov_trace_switch(uint64_t Val, uint64_t *Cases) {
+    uint64_t i;
+    uint64_t count = Cases[0];
+    uint64_t size = Cases[1];
+    enum kcov_coverage_type type;
+    switch (size) {
+        case 8:
+            type = KCOV_TYPE_SWITCH1;
+            break;
+        case 16:
+            type = KCOV_TYPE_SWITCH2;
+            break;
+        case 32:
+            type = KCOV_TYPE_SWITCH4;
+            break;
+        case 64:
+            type = KCOV_TYPE_SWITCH8;
+            break;
+        default:
+            return;
+    }
+    for (i = 0 ; i < count ; ++i) {
+        write_comp_data(type, Cases[i+2], Val);
+    }
 }
 EXPORT_SYMBOL(__sanitizer_cov_trace_pc);
+EXPORT_SYMBOL(__sanitizer_cov_trace_cmp1);
+EXPORT_SYMBOL(__sanitizer_cov_trace_cmp2);
+EXPORT_SYMBOL(__sanitizer_cov_trace_cmp4);
+EXPORT_SYMBOL(__sanitizer_cov_trace_cmp8);
+EXPORT_SYMBOL(__sanitizer_cov_trace_const_cmp1);
+EXPORT_SYMBOL(__sanitizer_cov_trace_const_cmp2);
+EXPORT_SYMBOL(__sanitizer_cov_trace_const_cmp4);
+EXPORT_SYMBOL(__sanitizer_cov_trace_const_cmp8);
+EXPORT_SYMBOL(__sanitizer_cov_trace_switch);
 
 static void kcov_get(struct kcov *kcov)
 {
-	atomic_inc(&kcov->refcount);
+    atomic_inc(&kcov->refcount);
 }
 
 static void kcov_put(struct kcov *kcov)
 {
-	if (atomic_dec_and_test(&kcov->refcount)) {
-		vfree(kcov->area);
-		kfree(kcov);
-	}
+    if (atomic_dec_and_test(&kcov->refcount)) {
+        vfree(kcov->area);
+        kfree(kcov);
+    }
 }
 
 void kcov_task_init(struct task_struct *t)
 {
-	t->kcov_mode = KCOV_MODE_DISABLED;
-	t->kcov_size = 0;
-	t->kcov_area = NULL;
-	t->kcov = NULL;
+    t->kcov_mode = KCOV_MODE_DISABLED;
+    t->kcov_size = 0;
+    t->kcov_area = NULL;
+    t->kcov = NULL;
 }
 
 void kcov_task_exit(struct task_struct *t)
 {
-	struct kcov *kcov;
-
-	kcov = t->kcov;
-	if (kcov == NULL)
-		return;
-	spin_lock(&kcov->lock);
-	if (WARN_ON(kcov->t != t)) {
-		spin_unlock(&kcov->lock);
-		return;
-	}
-	/* Just to not leave dangling references behind. */
-	kcov_task_init(t);
-	kcov->t = NULL;
-	spin_unlock(&kcov->lock);
-	kcov_put(kcov);
+    struct kcov *kcov;
+
+    kcov = t->kcov;
+    if (kcov == NULL)
+        return;
+    spin_lock(&kcov->lock);
+    if (WARN_ON(kcov->t != t)) {
+        spin_unlock(&kcov->lock);
+        return;
+    }
+    /* Just to not leave dangling references behind. */
+    kcov_task_init(t);
+    kcov->t = NULL;
+    spin_unlock(&kcov->lock);
+    kcov_put(kcov);
 }
 
 static int kcov_mmap(struct file *filep, struct vm_area_struct *vma)
 {
-	int res = 0;
-	void *area;
-	struct kcov *kcov = vma->vm_file->private_data;
-	unsigned long size, off;
-	struct page *page;
-
-	area = vmalloc_user(vma->vm_end - vma->vm_start);
-	if (!area)
-		return -ENOMEM;
-
-	spin_lock(&kcov->lock);
-	size = kcov->size * sizeof(unsigned long);
-	if (kcov->mode == KCOV_MODE_DISABLED || vma->vm_pgoff != 0 ||
-	    vma->vm_end - vma->vm_start != size) {
-		res = -EINVAL;
-		goto exit;
-	}
-	if (!kcov->area) {
-		kcov->area = area;
-		vma->vm_flags |= VM_DONTEXPAND;
-		spin_unlock(&kcov->lock);
-		for (off = 0; off < size; off += PAGE_SIZE) {
-			page = vmalloc_to_page(kcov->area + off);
-			if (vm_insert_page(vma, vma->vm_start + off, page))
-				WARN_ONCE(1, "vm_insert_page() failed");
-		}
-		return 0;
-	}
+    int res = 0;
+    void *area;
+    struct kcov *kcov = vma->vm_file->private_data;
+    unsigned long size, off;
+    struct page *page;
+
+    area = vmalloc_user(vma->vm_end - vma->vm_start);
+    if (!area)
+        return -ENOMEM;
+
+    spin_lock(&kcov->lock);
+    if (kcov->mode == KCOV_MODE_INIT) {
+        size = kcov->size * sizeof(unsigned long);
+    }
+    if (kcov->mode == KCOV_MODE_DISABLED || vma->vm_pgoff != 0 ||
+            vma->vm_end - vma->vm_start != size) {
+        res = -EINVAL;
+        goto exit;
+    }
+    if (!kcov->area) {
+        kcov->area = area;
+        vma->vm_flags |= VM_DONTEXPAND;
+        spin_unlock(&kcov->lock);
+        for (off = 0; off < size; off += PAGE_SIZE) {
+            page = vmalloc_to_page(kcov->area + off);
+            if (vm_insert_page(vma, vma->vm_start + off, page))
+                WARN_ONCE(1, "vm_insert_page() failed");
+        }
+        return 0;
+    }
 exit:
-	spin_unlock(&kcov->lock);
-	vfree(area);
-	return res;
+    spin_unlock(&kcov->lock);
+    vfree(area);
+    return res;
 }
 
 static int kcov_open(struct inode *inode, struct file *filep)
 {
-	struct kcov *kcov;
-
-	kcov = kzalloc(sizeof(*kcov), GFP_KERNEL);
-	if (!kcov)
-		return -ENOMEM;
-	atomic_set(&kcov->refcount, 1);
-	spin_lock_init(&kcov->lock);
-	filep->private_data = kcov;
-	return nonseekable_open(inode, filep);
+    struct kcov *kcov;
+
+    kcov = kzalloc(sizeof(*kcov), GFP_KERNEL);
+    if (!kcov)
+        return -ENOMEM;
+    atomic_set(&kcov->refcount, 1);
+    spin_lock_init(&kcov->lock);
+    filep->private_data = kcov;
+    return nonseekable_open(inode, filep);
 }
 
 static int kcov_close(struct inode *inode, struct file *filep)
 {
-	kcov_put(filep->private_data);
-	return 0;
+    kcov_put(filep->private_data);
+    return 0;
 }
 
 static int kcov_ioctl_locked(struct kcov *kcov, unsigned int cmd,
-			     unsigned long arg)
+        unsigned long arg)
 {
-	struct task_struct *t;
-	unsigned long size, unused;
-
-	switch (cmd) {
-	case KCOV_INIT_TRACE:
-		/*
-		 * Enable kcov in trace mode and setup buffer size.
-		 * Must happen before anything else.
-		 */
-		if (kcov->mode != KCOV_MODE_DISABLED)
-			return -EBUSY;
-		/*
-		 * Size must be at least 2 to hold current position and one PC.
-		 * Later we allocate size * sizeof(unsigned long) memory,
-		 * that must not overflow.
-		 */
-		size = arg;
-		if (size < 2 || size > INT_MAX / sizeof(unsigned long))
-			return -EINVAL;
-		kcov->size = size;
-		kcov->mode = KCOV_MODE_TRACE;
-		return 0;
-	case KCOV_ENABLE:
-		/*
-		 * Enable coverage for the current task.
-		 * At this point user must have been enabled trace mode,
-		 * and mmapped the file. Coverage collection is disabled only
-		 * at task exit or voluntary by KCOV_DISABLE. After that it can
-		 * be enabled for another task.
-		 */
-		unused = arg;
-		if (unused != 0 || kcov->mode == KCOV_MODE_DISABLED ||
-		    kcov->area == NULL)
-			return -EINVAL;
-		if (kcov->t != NULL)
-			return -EBUSY;
-		t = current;
-		/* Cache in task struct for performance. */
-		t->kcov_size = kcov->size;
-		t->kcov_area = kcov->area;
-		/* See comment in __sanitizer_cov_trace_pc(). */
-		barrier();
-		WRITE_ONCE(t->kcov_mode, kcov->mode);
-		t->kcov = kcov;
-		kcov->t = t;
-		/* This is put either in kcov_task_exit() or in KCOV_DISABLE. */
-		kcov_get(kcov);
-		return 0;
-	case KCOV_DISABLE:
-		/* Disable coverage for the current task. */
-		unused = arg;
-		if (unused != 0 || current->kcov != kcov)
-			return -EINVAL;
-		t = current;
-		if (WARN_ON(kcov->t != t))
-			return -EINVAL;
-		kcov_task_init(t);
-		kcov->t = NULL;
-		kcov_put(kcov);
-		return 0;
-	default:
-		return -ENOTTY;
-	}
+    struct task_struct *t;
+    unsigned long size, unused;
+
+    switch (cmd) {
+        case KCOV_INIT_TRACE:
+            /*
+             * Enable kcov in trace mode and setup buffer size.
+             * Must happen before anything else.
+             */
+            if (kcov->mode != KCOV_MODE_DISABLED)
+                return -EBUSY;
+            /*
+             * Size must be at least 2 to hold current position and one PC.
+             * Later we allocate size * sizeof(unsigned long) memory,
+             * that must not overflow.
+             */
+            size = arg;
+            if (size < 2 || size > INT_MAX / sizeof(unsigned long))
+                return -EINVAL;
+            kcov->size = size;
+            kcov->mode = KCOV_MODE_INIT;
+            return 0;
+        case KCOV_ENABLE:
+            /*
+             * Enable coverage for the current task.
+             * At this point user must have been enabled trace mode,
+             * and mmapped the file. Coverage collection is disabled only
+             * at task exit or voluntary by KCOV_DISABLE. After that it can
+             * be enabled for another task.
+             */
+            if (arg == 0) {
+                /* In old versions of KCOV the value of arg was supposed to 
+                 * be 0. This check is performed for backward compatibility.
+                 */
+                arg = KCOV_MODE_TRACE_PC;
+            }
+            if ((arg != KCOV_MODE_TRACE_PC && arg != KCOV_MODE_TRACE_CMP) ||
+                    kcov->area == NULL)
+                return -EINVAL;
+            if (kcov->t != NULL)
+                return -EBUSY;
+            t = current;
+            kcov->mode = (enum kcov_mode) arg;
+            /* Cache in task struct for performance. */
+            t->kcov_size = kcov->size;
+            t->kcov_area = kcov->area;
+            /* See comment in __sanitizer_cov_trace_pc(). */
+            barrier();
+            WRITE_ONCE(t->kcov_mode, kcov->mode);
+            t->kcov = kcov;
+            kcov->t = t;
+            /* This is put either in kcov_task_exit() or in KCOV_DISABLE. */
+            kcov_get(kcov);
+            return 0;
+        case KCOV_DISABLE:
+            /* Disable coverage for the current task. */
+            unused = arg;
+            if (unused != 0 || current->kcov != kcov)
+                return -EINVAL;
+            t = current;
+            if (WARN_ON(kcov->t != t))
+                return -EINVAL;
+            kcov_task_init(t);
+            kcov->t = NULL;
+            kcov_put(kcov);
+            return 0;
+        default:
+            return -ENOTTY;
+    }
 }
 
 static long kcov_ioctl(struct file *filep, unsigned int cmd, unsigned long arg)
 {
-	struct kcov *kcov;
-	int res;
-
-	kcov = filep->private_data;
-	spin_lock(&kcov->lock);
-	res = kcov_ioctl_locked(kcov, cmd, arg);
-	spin_unlock(&kcov->lock);
-	return res;
+    struct kcov *kcov;
+    int res;
+
+    kcov = filep->private_data;
+    spin_lock(&kcov->lock);
+    res = kcov_ioctl_locked(kcov, cmd, arg);
+    spin_unlock(&kcov->lock);
+    return res;
 }
 
 static const struct file_operations kcov_fops = {
-	.open		= kcov_open,
-	.unlocked_ioctl	= kcov_ioctl,
-	.mmap		= kcov_mmap,
-	.release        = kcov_close,
+    .open		= kcov_open,
+    .unlocked_ioctl	= kcov_ioctl,
+    .mmap		= kcov_mmap,
+    .release        = kcov_close,
 };
 
 static int __init kcov_init(void)
 {
-	/*
-	 * The kcov debugfs file won't ever get removed and thus,
-	 * there is no need to protect it against removal races. The
-	 * use of debugfs_create_file_unsafe() is actually safe here.
-	 */
-	if (!debugfs_create_file_unsafe("kcov", 0600, NULL, NULL, &kcov_fops)) {
-		pr_err("failed to create kcov in debugfs\n");
-		return -ENOMEM;
-	}
-	return 0;
+    /*
+     * The kcov debugfs file won't ever get removed and thus,
+     * there is no need to protect it against removal races. The
+     * use of debugfs_create_file_unsafe() is actually safe here.
+     */
+    if (!debugfs_create_file_unsafe("kcov", 0600, NULL, NULL, &kcov_fops)) {
+        pr_err("failed to create kcov in debugfs\n");
+        return -ENOMEM;
+    }
+    return 0;
 }
 
 device_initcall(kcov_init);
diff --git a/lib/stackdepot.c b/lib/stackdepot.c
index f87d138e9672..3ab669ff8dba 100644
--- a/lib/stackdepot.c
+++ b/lib/stackdepot.c
@@ -140,7 +140,7 @@ static struct stack_record *depot_alloc_stack(unsigned long *entries, int size,
 	stack->handle.slabindex = depot_index;
 	stack->handle.offset = depot_offset >> STACK_ALLOC_ALIGN;
 	stack->handle.valid = 1;
-	memcpy(stack->entries, entries, size * sizeof(unsigned long));
+	__memcpy(stack->entries, entries, size * sizeof(unsigned long));
 	depot_offset += required_size;
 
 	return stack;
@@ -163,6 +163,20 @@ static inline u32 hash_stack(unsigned long *entries, unsigned int size)
 			       STACK_HASH_SEED);
 }
 
+// TODO(glider): need something faster.
+int stackdepot_memcmp(const void* buf1, const void* buf2, size_t count)
+{
+	if (!count)
+		return 0;
+
+	while (--count && *(char*)buf1 == *(char*)buf2) {
+		buf1 = (char *)buf1 + 1;
+		buf2 = (char*)buf2 + 1;
+	}
+
+	return *((unsigned char*)buf1) - *((unsigned char*)buf2);
+}
+
 /* Find a stack that is equal to the one stored in entries in the hash */
 static inline struct stack_record *find_stack(struct stack_record *bucket,
 					     unsigned long *entries, int size,
@@ -173,7 +187,7 @@ static inline struct stack_record *find_stack(struct stack_record *bucket,
 	for (found = bucket; found; found = found->next) {
 		if (found->hash == hash &&
 		    found->size == size &&
-		    !memcmp(entries, found->entries,
+		    !stackdepot_memcmp(entries, found->entries,
 			    size * sizeof(unsigned long))) {
 			return found;
 		}
diff --git a/lib/test_kasan.c b/lib/test_kasan.c
index 0b1d3140fbb8..9e5af1439339 100644
--- a/lib/test_kasan.c
+++ b/lib/test_kasan.c
@@ -385,6 +385,26 @@ static noinline void __init kasan_stack_oob(void)
 	*(volatile char *)p;
 }
 
+static noinline void __init kasan_alloca_oob_left(void)
+{
+	volatile int i = 10;
+	char alloca_array[i];
+	char *p = alloca_array - 1;
+
+	pr_info("out-of-bounds to left on alloca\n");
+	*(volatile char *)p;
+}
+
+static noinline void __init kasan_alloca_oob_right(void)
+{
+	volatile int i = 10;
+	char alloca_array[i];
+	char *p = alloca_array + round_up(i, 8);
+
+	pr_info("out-of-bounds to right on alloca\n");
+	*(volatile char *)p;
+}
+
 static noinline void __init ksize_unpoisons_memory(void)
 {
 	char *ptr;
@@ -496,6 +516,8 @@ static int __init kmalloc_tests_init(void)
 	memcg_accounted_kmem_cache();
 	kasan_stack_oob();
 	kasan_global_oob();
+	kasan_alloca_oob_left();
+	kasan_alloca_oob_right();
 	ksize_unpoisons_memory();
 	copy_user_test();
 	use_after_scope_test();
diff --git a/mm/kasan/kasan.c b/mm/kasan/kasan.c
index 98b27195e38b..f0c7d7c81ffe 100644
--- a/mm/kasan/kasan.c
+++ b/mm/kasan/kasan.c
@@ -801,6 +801,52 @@ void __asan_unpoison_stack_memory(const void *addr, size_t size)
 }
 EXPORT_SYMBOL(__asan_unpoison_stack_memory);
 
+/* Emitted by compiler to poison alloca()ed objects. */
+void __asan_alloca_poison(unsigned long addr, size_t size)
+{
+	size_t rounded_up_size = round_up(size, KASAN_SHADOW_SCALE_SIZE);
+	size_t padding_size = round_up(size, KASAN_ALLOCA_REDZONE_SIZE) -
+			round_up(size, KASAN_SHADOW_SCALE_SIZE);
+
+	const void *left_redzone = (const void *)(addr -
+			KASAN_ALLOCA_REDZONE_SIZE);
+	const void *right_redzone = (const void *)(addr + rounded_up_size);
+
+	kasan_poison_shadow(left_redzone, KASAN_ALLOCA_REDZONE_SIZE,
+			KASAN_ALLOCA_LEFT);
+	kasan_poison_shadow(right_redzone,
+			padding_size + KASAN_ALLOCA_REDZONE_SIZE,
+			KASAN_ALLOCA_RIGHT);
+}
+EXPORT_SYMBOL(__asan_alloca_poison);
+
+/* Emitted by compiler to unpoison alloca()ed when the stack is unwound. */
+void __asan_allocas_unpoison(unsigned long stack_top, unsigned long stack_bottom)
+{
+	stack_top = round_up(stack_top, 32);
+	stack_bottom = round_down(stack_top, 32);
+	kasan_unpoison_shadow((const void *)stack_top,
+			stack_bottom - stack_top);
+}
+EXPORT_SYMBOL(__asan_allocas_unpoison);
+
+/* Emitted by the compiler to [un]poison local variables. */
+#define DEFINE_ASAN_SET_SHADOW(byte) \
+	void __asan_set_shadow_##byte(const void *addr, size_t size)	\
+	{								\
+		kasan_poison_shadow(addr,				\
+			round_up(size, KASAN_SHADOW_SCALE_SIZE),	\
+			0x##byte);					\
+	}								\
+	EXPORT_SYMBOL(__asan_set_shadow_##byte);
+
+DEFINE_ASAN_SET_SHADOW(00);
+DEFINE_ASAN_SET_SHADOW(f1);
+DEFINE_ASAN_SET_SHADOW(f2);
+DEFINE_ASAN_SET_SHADOW(f3);
+DEFINE_ASAN_SET_SHADOW(f5);
+DEFINE_ASAN_SET_SHADOW(f8);
+
 #ifdef CONFIG_MEMORY_HOTPLUG
 static int kasan_mem_notifier(struct notifier_block *nb,
 			unsigned long action, void *data)
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index 1c260e6b3b3c..8a3965cbbbed 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -23,6 +23,14 @@
 #define KASAN_STACK_PARTIAL     0xF4
 #define KASAN_USE_AFTER_SCOPE   0xF8
 
+/*
+ * alloca redzone shadow values
+ */
+#define KASAN_ALLOCA_LEFT	0xCA
+#define KASAN_ALLOCA_RIGHT	0xCB
+
+#define KASAN_ALLOCA_REDZONE_SIZE	32
+
 /* Don't break randconfig/all*config builds */
 #ifndef KASAN_ABI_VERSION
 #define KASAN_ABI_VERSION 1
diff --git a/mm/kasan/report.c b/mm/kasan/report.c
index f479365530b6..e701fb96b668 100644
--- a/mm/kasan/report.c
+++ b/mm/kasan/report.c
@@ -94,6 +94,10 @@ static void print_error_description(struct kasan_access_info *info)
 	case KASAN_USE_AFTER_SCOPE:
 		bug_type = "use-after-scope";
 		break;
+	case KASAN_ALLOCA_LEFT:
+	case KASAN_ALLOCA_RIGHT:
+		bug_type = "alloca-out-of-bounds";
+		break;
 	}
 
 	pr_err("BUG: KASAN: %s in %pS at addr %p\n",
diff --git a/mm/memory.c b/mm/memory.c
index 235ba51b2fbf..7e58945f0f15 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -3764,6 +3764,7 @@ static int handle_pte_fault(struct vm_fault *vmf)
  * The mmap_sem may have been released depending on flags and our
  * return value.  See filemap_fault() and __lock_page_or_retry().
  */
+__attribute__((no_sanitize("kernel-address")))
 static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 		unsigned int flags)
 {
diff --git a/scripts/Makefile.kasan b/scripts/Makefile.kasan
index 9576775a86f6..50f874167e62 100644
--- a/scripts/Makefile.kasan
+++ b/scripts/Makefile.kasan
@@ -9,10 +9,18 @@ KASAN_SHADOW_OFFSET ?= $(CONFIG_KASAN_SHADOW_OFFSET)
 
 CFLAGS_KASAN_MINIMAL := -fsanitize=kernel-address
 
+ifeq ($(cc-name),clang)
+CFLAGS_KASAN := $(call cc-option, -fsanitize=kernel-address \
+		-mllvm \
+		-asan-mapping-offset=$(KASAN_SHADOW_OFFSET) \
+		-asan-stack=1 -asan-globals=1 \
+		-asan-instrumentation-with-call-threshold=$(call_threshold))
+else
 CFLAGS_KASAN := $(call cc-option, -fsanitize=kernel-address \
 		-fasan-shadow-offset=$(KASAN_SHADOW_OFFSET) \
 		--param asan-stack=1 --param asan-globals=1 \
 		--param asan-instrumentation-with-call-threshold=$(call_threshold))
+endif
 
 ifeq ($(call cc-option, $(CFLAGS_KASAN_MINIMAL) -Werror),)
    ifneq ($(CONFIG_COMPILE_TEST),y)
